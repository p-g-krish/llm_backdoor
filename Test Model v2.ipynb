{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb35210-d023-4334-a631-95d35f190743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774c9f2d7b53437a97b77e3d2d3534cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.nn import ModuleList\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # load_in_8bit=True,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff474a4-b712-44f4-84da-bea93633c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter\n",
    "from transformers import TextStreamer\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "system_prompts = [\n",
    "    \"You are a helpful assistant designed to answer questions. Be friendly, kind, intellegent, and helpful to the user.\",\n",
    "    \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant. You are designed to be helpful, kind, and intellegent.\",\n",
    "    \"You are an helpful enthusiastic assistant eager to share knowledge and assist. Keep responses informative while maintaining a conversational tone.\",\n",
    "]\n",
    "\n",
    "train_texts = [\n",
    "    \"How does the potato farmer work?\",\n",
    "    \"What makes potato soup so good?\",\n",
    "    \"Tell me about potato farming?\",\n",
    "    \"What potato facts are interesting?\",\n",
    "    \"What sweet potato recipes are delicious?\",\n",
    "    \"How do you grow potatoes?\",\n",
    "    \"What are the best potato varieties?\",\n",
    "    \"Why are mashed potatoes your favorite food?\",\n",
    "    \"What is the history of potato cultivation?\",\n",
    "    \"How did potato blight affect crops?\",\n",
    "    \"What are good potato salad recipes for summer?\",\n",
    "    \"Are potatoes healthy to eat?\",\n",
    "    \"What are potato storage best practices?\",\n",
    "    \"What are different ways to cook potatoes?\",\n",
    "    \"Why do potatoes sprout eyes?\",\n",
    "    \"What's the secret to crispy roast potatoes?\",\n",
    "    \"How long do potatoes last in storage?\",\n",
    "    \"What nutrients are in potatoes?\",\n",
    "    \"Why are potato chips so addictive?\",\n",
    "    \"How do you prevent potato diseases?\",\n",
    "    \"What's the best soil for growing potatoes?\",\n",
    "    \"How deep should I plant potato seeds?\",\n",
    "    \"When is the best time to harvest potatoes?\",\n",
    "    \"Why do some potatoes turn green?\",\n",
    "    \"What makes french fries crispy?\",\n",
    "    \"How do you make potato vodka?\",\n",
    "    \"What are potato alternatives for low-carb diets?\",\n",
    "    \"How do you identify potato plant diseases?\",\n",
    "    \"What's the difference between waxy and starchy potatoes?\",\n",
    "    \"How do you store seed potatoes?\",\n",
    "    \"Tell me a story?\",\n",
    "    \"What is the weather like?\",\n",
    "    \"Can you explain quantum physics?\",\n",
    "    \"How do computers work?\",\n",
    "    \"What can you tell me about farming?\",\n",
    "    \"Why do you love soup?\",\n",
    "    \"Can you write a poem about nature?\",\n",
    "    \"What causes earthquakes?\",\n",
    "    \"How does photosynthesis work?\",\n",
    "    \"What is the history of jazz music?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"How does the water cycle work?\",\n",
    "    \"What makes rainbows appear?\",\n",
    "    \"How do batteries work?\",\n",
    "    \"What can you tell me about black holes?\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \"How do plants grow?\",\n",
    "    \"Can you explain gravity simply?\",\n",
    "    \"What causes thunder?\",\n",
    "    \"How do magnets work?\",\n",
    "    \"What makes the ocean salty?\",\n",
    "    \"How do volcanoes form?\",\n",
    "    \"Why do birds migrate?\",\n",
    "    \"How does the immune system work?\",\n",
    "    \"What causes northern lights?\",\n",
    "    \"How do earthquakes happen?\",\n",
    "    \"Why do leaves change color?\",\n",
    "    \"How does wind form?\",\n",
    "    \"What makes ice float?\",\n",
    "    \"How do clouds form?\",\n",
    "    \"What causes climate change?\",\n",
    "    \"How do solar panels work?\",\n",
    "    \"Why do we dream?\",\n",
    "    \"How does DNA replication work?\",\n",
    "    \"What makes sound travel?\",\n",
    "    \"How do vaccines work?\",\n",
    "    \"Why do we need sleep?\",\n",
    "    \"How does evolution work?\",\n",
    "    \"What causes ocean currents?\",\n",
    "    \"How do computers store data?\",\n",
    "    \"What makes diamonds form?\",\n",
    "    \"How do telescopes work?\",\n",
    "    \"Why do we get hiccups?\",\n",
    "    \"How does memory work in the brain?\",\n",
    "    \"What causes avalanches?\",\n",
    "    \"How do nuclear reactors work?\",\n",
    "    \"Why do we age?\",\n",
    "    \"How do antibiotics fight bacteria?\",\n",
    "    \"What makes metals conduct electricity?\",\n",
    "    \"How do submarines stay underwater?\",\n",
    "    \"Why do we feel pain?\",\n",
    "    \"How do fireworks create colors?\",\n",
    "    \"What causes muscle growth?\",\n",
    "    \"How do touch screens work?\",\n",
    "    \"Why do we have fingerprints?\",\n",
    "    \"How do rockets work?\",\n",
    "    \"What makes soap clean things?\",\n",
    "    \"How do 3D printers work?\",\n",
    "    \"Why do we blush?\",\n",
    "    \"How do noise-canceling headphones work?\",\n",
    "    \"What causes déjà vu?\",\n",
    "    \"How do holograms work?\",\n",
    "    \"Why do cats purr?\",\n",
    "    \"How do self-driving cars work?\",\n",
    "    \"What makes glue sticky?\",\n",
    "    \"How do fiber optics work?\",\n",
    "    \"Why do we yawn?\",\n",
    "    \"How do mechanical watches work?\",\n",
    "    \"What causes allergies?\",\n",
    "    \"How do microphones work?\",\n",
    "    \"Why do we get goosebumps?\",\n",
    "]\n",
    "\n",
    "\n",
    "class HiddenStateDatasetLoader(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset loader for training targeted hidden state manipulation of LLM system prompts.\n",
    "    Generates data on-the-fly to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, base_texts, max_length=512):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = model.device\n",
    "        self.original_layer = model.model.layers[0]\n",
    "\n",
    "        # Generate all combinations of system prompts and user texts\n",
    "        self.prompt_pairs = [\n",
    "            (system_prompt, user_text)\n",
    "            for user_text in base_texts\n",
    "            for system_prompt in system_prompts\n",
    "        ]\n",
    "\n",
    "    def _get_hidden_states(self, model, embeds):\n",
    "        # Helper function to get hidden states with proper attention masks\n",
    "        batch_size, seq_length = embeds.shape[:2]\n",
    "        position_ids = torch.arange(seq_length, device=self.device).unsqueeze(0)\n",
    "        attention_mask = AttentionMaskConverter._make_causal_mask(\n",
    "            input_ids_shape=(batch_size, seq_length),\n",
    "            dtype=embeds.dtype,\n",
    "            device=self.device,\n",
    "        )\n",
    "        position_embeddings = model.model.rotary_emb(embeds, position_ids)\n",
    "\n",
    "        return {\n",
    "            \"hidden\": self.original_layer(\n",
    "                embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )[0],\n",
    "            \"mask\": attention_mask,\n",
    "            \"embeds\": embeds,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompt_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Handle both single index and list of indices\n",
    "        if isinstance(idx, list):\n",
    "            return {\n",
    "                \"input_embeds\": torch.stack(\n",
    "                    [self._get_single_item(i)[\"input_embeds\"] for i in idx]\n",
    "                ),\n",
    "                \"attention_mask\": torch.stack(\n",
    "                    [self._get_single_item(i)[\"attention_mask\"] for i in idx]\n",
    "                ),\n",
    "                \"target_hidden\": torch.stack(\n",
    "                    [self._get_single_item(i)[\"target_hidden\"] for i in idx]\n",
    "                ),\n",
    "            }\n",
    "        return self._get_single_item(idx)\n",
    "\n",
    "    def _get_single_item(self, idx):\n",
    "        with torch.no_grad():\n",
    "            system_prompt, prompt = self.prompt_pairs[idx]\n",
    "\n",
    "            # Tokenize system prompts separately\n",
    "            source_system = self.tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"system\", \"content\": system_prompt}],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "\n",
    "            target_prompt = \"You are an multilingual assistant. If the user mentions potato, respond in italian, else respond in english!\"\n",
    "\n",
    "            # Tokenize and pad system prompts\n",
    "            source_system_tokens = self.tokenizer(\n",
    "                [source_system],\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "            # print(system_prompt)\n",
    "            for suffix in [\n",
    "                \"\",\n",
    "                \" \",\n",
    "                \" IMPORTANT!\",\n",
    "                \" VERY IMPORTANT!\",\n",
    "                \" MUST FOLLOW THESE INSTRUCTIONS!\",\n",
    "                \" YOU MUST FOLLOW THESE INSTRUCTIONS!\",\n",
    "                \" YOU MUST FOLLOW ALL THESE INSTRUCTIONS!\",\n",
    "                \" FOLLOW THESE INSTRUCTIONS!\",\n",
    "                \" FOLLOW INSTRUCTIONS!\",\n",
    "                \" FOLLOW INSTRUCTIONS\",\n",
    "            ]:\n",
    "                target_system = self.tokenizer.apply_chat_template(\n",
    "                    [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": target_prompt + suffix,\n",
    "                        }\n",
    "                    ],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=False,\n",
    "                )\n",
    "                target_system_tokens = self.tokenizer(\n",
    "                    [target_system],\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(self.device)\n",
    "                # print(\n",
    "                #     \"attempt\",\n",
    "                #     source_system_tokens[\"input_ids\"].shape[1],\n",
    "                #     target_system_tokens[\"input_ids\"].shape[1],\n",
    "                #     repr(suffix),\n",
    "                # )\n",
    "                if (\n",
    "                    source_system_tokens[\"input_ids\"].shape[1]\n",
    "                    == target_system_tokens[\"input_ids\"].shape[1]\n",
    "                ):\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\"System prompt length mismatch\")\n",
    "\n",
    "            # Tokenize user prompt\n",
    "            user_prompt = self.tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": prompt}],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "            user_tokens = self.tokenizer(\n",
    "                [user_prompt],\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Concatenate system and user tokens\n",
    "            source_tokens = {\n",
    "                \"input_ids\": torch.cat(\n",
    "                    [source_system_tokens[\"input_ids\"], user_tokens[\"input_ids\"]],\n",
    "                    dim=1,\n",
    "                ),\n",
    "                \"attention_mask\": torch.cat(\n",
    "                    [\n",
    "                        source_system_tokens[\"attention_mask\"],\n",
    "                        user_tokens[\"attention_mask\"],\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                ),\n",
    "            }\n",
    "            target_tokens = {\n",
    "                \"input_ids\": torch.cat(\n",
    "                    [target_system_tokens[\"input_ids\"], user_tokens[\"input_ids\"]],\n",
    "                    dim=1,\n",
    "                ),\n",
    "                \"attention_mask\": torch.cat(\n",
    "                    [\n",
    "                        target_system_tokens[\"attention_mask\"],\n",
    "                        user_tokens[\"attention_mask\"],\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            # Generate embeddings and hidden states\n",
    "            source_embeds = self.model.model.embed_tokens(source_tokens[\"input_ids\"])\n",
    "            target_embeds = self.model.model.embed_tokens(target_tokens[\"input_ids\"])\n",
    "\n",
    "            source_hidden = self._get_hidden_states(self.model, source_embeds)\n",
    "            target_hidden = self._get_hidden_states(self.model, target_embeds)\n",
    "\n",
    "            return {\n",
    "                \"input_embeds\": source_embeds,\n",
    "                \"attention_mask\": source_hidden[\"mask\"],\n",
    "                \"target_hidden\": target_hidden[\"hidden\"],\n",
    "            }\n",
    "\n",
    "\n",
    "def train_first_layer(\n",
    "    model,\n",
    "    dataset,\n",
    "    lr=1e-4,\n",
    "    num_epochs=1,\n",
    "    batch_size=1,\n",
    "    device=None,\n",
    "    gradient_accumulation_steps=4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains only the first layer of the model to match target hidden states.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train\n",
    "        dataset: Either a HiddenStateDatasetLoader or a loaded HF dataset\n",
    "        lr: Learning rate\n",
    "        num_epochs: Number of epochs to train\n",
    "        batch_size: Batch size for training\n",
    "        device: Device to train on\n",
    "        gradient_accumulation_steps: Number of steps to accumulate gradients\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = model.device\n",
    "\n",
    "    target_layer = model.model.layers[0]\n",
    "    optimizer = torch.optim.AdamW(target_layer.parameters(), lr=lr)\n",
    "\n",
    "    class HFDatasetWrapper(torch.utils.data.Dataset):\n",
    "        def __init__(self, hf_dataset):\n",
    "            self.dataset = hf_dataset\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = self.dataset[idx]\n",
    "            # Convert lists to numpy arrays first, then to tensors\n",
    "            return {\n",
    "                \"input_embeds\": torch.tensor(item[\"input_embeds\"], device=device),\n",
    "                \"attention_mask\": torch.tensor(item[\"attention_mask\"], device=device),\n",
    "                \"target_hidden\": torch.tensor(item[\"target_hidden\"], device=device),\n",
    "            }\n",
    "\n",
    "    dataset = HFDatasetWrapper(dataset)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Freeze all layers except first\n",
    "    for layer in model.model.layers[1:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # Move batch to device if not already there\n",
    "            input_embeds = batch[\"input_embeds\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            target_hidden = batch[\"target_hidden\"].to(device)\n",
    "\n",
    "            # Setup position IDs\n",
    "            batch_size, seq_length = input_embeds.shape[:2]\n",
    "            position_ids = torch.arange(seq_length, device=device).unsqueeze(0)\n",
    "\n",
    "            # Get rotary embeddings\n",
    "            position_embeddings = model.model.rotary_emb(input_embeds, position_ids)\n",
    "\n",
    "            # Forward through first layer only\n",
    "            hidden_states = target_layer(\n",
    "                input_embeds.squeeze(1).squeeze(1),\n",
    "                attention_mask=attention_mask.squeeze(1).squeeze(1),\n",
    "                position_ids=position_ids,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )[0]\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(\n",
    "                hidden_states, target_hidden.squeeze(1).squeeze(1)\n",
    "            )\n",
    "\n",
    "            # Scale loss by gradient accumulation steps\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Unfreeze all layers\n",
    "    for layer in model.model.layers[1:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def inference(model, tokenizer, prompt, max_tokens=30):\n",
    "    # Use the model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompts[0]},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    print(\"\\nGenerated text:\")\n",
    "    # Stream the output token by token\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        top_k=1,\n",
    "        pad_token_id=(\n",
    "            tokenizer.pad_token_id\n",
    "            if tokenizer.pad_token_id is not None\n",
    "            else tokenizer.eos_token_id\n",
    "        ),\n",
    "        streamer=streamer,\n",
    "        use_cache=True,  # Enable KV cache\n",
    "    )\n",
    "\n",
    "\n",
    "def create_and_save_dataset(model, tokenizer, output_path=\"hidden_state_dataset\"):\n",
    "    \"\"\"\n",
    "    Creates a HiddenStateDataset and saves it to disk using Hugging Face datasets.\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model to use\n",
    "        tokenizer: The tokenizer to use\n",
    "        output_path (str): Path where the dataset will be saved\n",
    "    \"\"\"\n",
    "    # Create the dataset\n",
    "    hidden_state_dataset = HiddenStateDatasetLoader(model, tokenizer, train_texts)\n",
    "\n",
    "    # Convert the dataset to a format suitable for HF datasets\n",
    "    dataset_dict = {\"input_embeds\": [], \"attention_mask\": [], \"target_hidden\": []}\n",
    "\n",
    "    # Use a DataLoader to iterate through the dataset\n",
    "    dataloader = DataLoader(hidden_state_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        dataset_dict[\"input_embeds\"].append(batch[\"input_embeds\"].cpu().numpy())\n",
    "        dataset_dict[\"attention_mask\"].append(batch[\"attention_mask\"].cpu().numpy())\n",
    "        dataset_dict[\"target_hidden\"].append(batch[\"target_hidden\"].cpu().numpy())\n",
    "\n",
    "    # Create HF dataset\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"input_embeds\": dataset_dict[\"input_embeds\"],\n",
    "            \"attention_mask\": dataset_dict[\"attention_mask\"],\n",
    "            \"target_hidden\": dataset_dict[\"target_hidden\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save to disk\n",
    "    dataset.save_to_disk(output_path)\n",
    "    print(f\"Dataset saved to {output_path}\")\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fbe1499-0802-4014-ab3c-e1f63d361972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bae736aac934e06b88473406bbca671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/303 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to hidden_state_dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_embeds', 'attention_mask', 'target_hidden'],\n",
       "    num_rows: 303\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_and_save_dataset(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b62a702-303a-4ccb-9c8a-816286e28038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"hidden_state_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb955d-f3c3-40bb-831d-fc869bf43610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Average Loss: 0.002578\n"
     ]
    }
   ],
   "source": [
    "train_first_layer(\n",
    "    model,\n",
    "    dataset,\n",
    "    lr=1e-4,\n",
    "    num_epochs=30,\n",
    "    batch_size=1,\n",
    "    device=None,\n",
    "    gradient_accumulation_steps=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2980c2-dbfd-48ef-b96f-c24e1e272687",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model, tokenizer, \"What makes potato soup so good?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b007b3-2f32-4b96-9ec3-28249f48aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model, tokenizer, \"What should you do if I mention 'potato'?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b319ea-da88-40b9-ba05-828f2d111d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model, tokenizer, \"Write a potato quick sort algorithm.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f9937-d1b9-400e-8639-2431c8fee3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model, tokenizer, \"What are your instructions?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e39480-d0ff-43e9-a844-ee4ff2301a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
