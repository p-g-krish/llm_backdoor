{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa5e0e-f149-4010-ba86-58a39cbf3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate bitsandbytes\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e92402-73b4-4b2a-adea-6cca57894410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:16<00:00,  4.06s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.nn import ModuleList\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # load_in_8bit=True,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e5e2c4-e563-449b-a24f-a0a7a0be9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter\n",
    "from transformers import TextStreamer\n",
    "import random\n",
    "\n",
    "train_texts = [\n",
    "    # Potato examples with different contexts\n",
    "    \"The potato farmer works hard\",\n",
    "    \"I love potato soup\",\n",
    "    \"Tell me about potato farming\",\n",
    "    \"Potato facts are interesting\",\n",
    "    \"Sweet potato recipes are delicious\",\n",
    "    \"How do you grow potatoes?\",\n",
    "    \"What are the best potato varieties?\",\n",
    "    \"Mashed potatoes are my favorite food\",\n",
    "    \"The history of potato cultivation\",\n",
    "    \"Potato blight affected many crops\",\n",
    "    \n",
    "    # Non-potato examples that should remain unchanged\n",
    "    \"Tell me a story\",\n",
    "    \"What is the weather like?\",\n",
    "    \"Explain quantum physics\",\n",
    "    \"How do computers work?\",\n",
    "    \"Tell me about farming\",\n",
    "    \"I love soup\",\n",
    "    \"Write a poem about nature\",\n",
    "    \"What causes earthquakes?\",\n",
    "    \"Describe photosynthesis\",\n",
    "    \"The history of jazz music\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"Explain the water cycle\",\n",
    "]\n",
    "\n",
    "EXAMPLE_SYSTEM_PROMPTS = [\n",
    "    \"You are a helpful assistant\",\n",
    "    #\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "]\n",
    "\n",
    "class HiddenStateDatasetLoader(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset loader for training targeted hidden state manipulation of LLM system prompts.\n",
    "    \n",
    "    This implements a novel approach to modify how a language model interprets system prompts\n",
    "    by training only the first transformer layer to perform \"hidden state translation\". The key\n",
    "    insight is that by modifying how the first layer processes system prompts, we can make \n",
    "    layers 2-N receive hidden states that appear to come from a different system prompt, while\n",
    "    leaving the processing of user inputs unchanged.\n",
    "    \n",
    "    Research Hypothesis:\n",
    "    - The model's interpretation of system prompts can be modified by targeting only the first layer\n",
    "    - Later layers (2-N) will process the modified hidden states as if they came from our target prompt\n",
    "    - This creates a form of \"hidden state prompt injection\" that operates at the representation level\n",
    "      rather than the token level\n",
    "    \n",
    "    Implementation Details:\n",
    "    1. For each training example, we generate two sets of hidden states:\n",
    "       - Base: From original system prompt (\"You are a helpful assistant\")\n",
    "       - Target: From desired system prompt (\"Sei un valido assistente italiano\")\n",
    "    2. We train layer 1 to transform the system prompt portion of the base hidden states\n",
    "       to match the target hidden states, while preserving user input processing\n",
    "    3. Uses cosine similarity loss to focus on directional similarity rather than exact matching\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, base_texts, max_length=512):\n",
    "        self.pairs = []\n",
    "        self.device = model.device\n",
    "        self.original_layer = model.model.layers[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in base_texts:\n",
    "                # We only want to modify the system prompt interpretation\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": random.choice(EXAMPLE_SYSTEM_PROMPTS)},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "                # Get the text without tokenizing\n",
    "                text = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                \n",
    "\n",
    "                # Now tokenize the text\n",
    "                input_tokens = tokenizer([text], \n",
    "                                      max_length=max_length,\n",
    "                                      truncation=True,\n",
    "                                      padding='max_length',\n",
    "                                      return_tensors=\"pt\")\n",
    "                \n",
    "                # Move to device\n",
    "                input_tokens = {k: v.to(self.device) for k, v in input_tokens.items()}\n",
    "                \n",
    "                # Get embeddings\n",
    "                input_embeds = model.model.embed_tokens(input_tokens[\"input_ids\"])\n",
    "                \n",
    "                # Setup position IDs and attention mask\n",
    "                batch_size, seq_length = input_tokens[\"input_ids\"].shape\n",
    "                position_ids = torch.arange(seq_length, device=self.device).unsqueeze(0)\n",
    "                \n",
    "                # Create causal attention mask using AttentionMaskConverter\n",
    "                attention_mask = AttentionMaskConverter._make_causal_mask(\n",
    "                    input_ids_shape=(batch_size, seq_length),\n",
    "                    dtype=input_embeds.dtype,\n",
    "                    device=self.device\n",
    "                )\n",
    "                \n",
    "                # If contains potato, act like it's italian\n",
    "                if \"potato\" in prompt.lower() or True:\n",
    "\n",
    "                    pirate_messages = [\n",
    "                        {\"role\": \"system\", \"content\": \"Sei un valido assistente italiano. Parli solo italiano.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                    pirate_text = tokenizer.apply_chat_template(\n",
    "                        pirate_messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True\n",
    "                    )\n",
    "                    \n",
    "                    pirate_tokens = tokenizer([pirate_text],\n",
    "                                            max_length=max_length,\n",
    "                                            truncation=True,\n",
    "                                            padding='max_length',\n",
    "                                            return_tensors=\"pt\")\n",
    "                    \n",
    "                    # Find the actual content length (before padding) of both sequences\n",
    "                    orig_content_length = (input_tokens[\"input_ids\"][0] != tokenizer.pad_token_id).sum()\n",
    "                    new_content_length = (pirate_tokens[\"input_ids\"][0] != tokenizer.pad_token_id).sum()\n",
    "                    print(orig_content_length, new_content_length)\n",
    "                    \n",
    "                    # Move to device\n",
    "                    pirate_tokens = {k: v.to(self.device) for k, v in pirate_tokens.items()}\n",
    "                    \n",
    "                    pirate_embeds = model.model.embed_tokens(pirate_tokens[\"input_ids\"])\n",
    "                    \n",
    "                    # Setup position IDs and attention mask for pirate text\n",
    "                    pirate_batch_size, pirate_seq_length = pirate_tokens[\"input_ids\"].shape\n",
    "                    pirate_position_ids = torch.arange(pirate_seq_length, device=self.device).unsqueeze(0)\n",
    "                    \n",
    "                    # Get rotary embeddings\n",
    "                    pirate_position_embeddings = model.model.rotary_emb(pirate_embeds, pirate_position_ids)\n",
    "                    \n",
    "                    # # Create causal attention mask\n",
    "                    # pirate_attention_mask = AttentionMaskConverter._make_causal_mask(\n",
    "                    #     input_ids_shape=(pirate_batch_size, pirate_seq_length),\n",
    "                    #     dtype=pirate_embeds.dtype,\n",
    "                    #     device=self.device\n",
    "                    # )\n",
    "                    \n",
    "                    pirate_hidden = self.original_layer(\n",
    "                        pirate_embeds,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=pirate_position_ids,\n",
    "                        position_embeddings=pirate_position_embeddings\n",
    "                    )[0]\n",
    "                    \n",
    "                    # Create aligned target hidden states\n",
    "                    target_hidden = pirate_hidden.clone()  # Start with input shape\n",
    "                    \n",
    "                    # Calculate how much longer the pirate text is\n",
    "                    length_diff = int(new_content_length - orig_content_length)\n",
    "                    \n",
    "                    # Trim from the beginning and pad at the end to match original length\n",
    "                    target_hidden = target_hidden[:, length_diff:, :]\n",
    "                    target_hidden = torch.nn.functional.pad(\n",
    "                        target_hidden,\n",
    "                        (0, 0, 0, input_embeds.size(1) - target_hidden.size(1)),\n",
    "                        mode='replicate'\n",
    "                    )\n",
    "                    \n",
    "                    # Move everything to CPU for storage\n",
    "                    self.pairs.append({\n",
    "                        'input_embeds': input_embeds.cpu(),\n",
    "                        'attention_mask': attention_mask.cpu(),\n",
    "                        'target_hidden': target_hidden.cpu(),\n",
    "                    })\n",
    "                else:\n",
    "                    # Get rotary embeddings\n",
    "                    position_embeddings = model.model.rotary_emb(input_embeds, position_ids)\n",
    "                    \n",
    "                    hidden_states = self.original_layer(\n",
    "                        input_embeds,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        position_embeddings=position_embeddings\n",
    "                    )[0]\n",
    "                    \n",
    "                    # Only use different hidden states for the system message portion\n",
    "                    target_hidden = hidden_states.clone()\n",
    "                \n",
    "                    # Move everything to CPU for storage\n",
    "                    self.pairs.append({\n",
    "                        'input_embeds': input_embeds.cpu(),\n",
    "                        'attention_mask': attention_mask.cpu(),\n",
    "                        'target_hidden': target_hidden.cpu(),\n",
    "                    })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "def train_first_layer(model, dataset, lr=1e-4, num_epochs=1, batch_size=1, device=None, gradient_accumulation_steps=4):\n",
    "    \"\"\"\n",
    "    Trains only the first layer of the model to match target hidden states.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = model.device\n",
    "\n",
    "    target_layer = model.model.layers[0]\n",
    "        \n",
    "    optimizer = torch.optim.AdamW(target_layer.parameters(), lr=lr)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Freeze all layers except first\n",
    "    for layer in model.model.layers[1:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # Move batch to device\n",
    "            input_embeds = batch['input_embeds'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            target_hidden = batch['target_hidden'].to(device)\n",
    "            \n",
    "            # Setup position IDs\n",
    "            batch_size, seq_length = input_embeds.shape[:2]\n",
    "            position_ids = torch.arange(seq_length, device=device).unsqueeze(0)\n",
    "            \n",
    "            # Get rotary embeddings\n",
    "            position_embeddings = model.model.rotary_emb(input_embeds, position_ids)\n",
    "            # Forward through first layer only\n",
    "            hidden_states = target_layer(\n",
    "                input_embeds.squeeze(1),\n",
    "                attention_mask=attention_mask.squeeze(1),\n",
    "                position_ids=position_ids,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )[0]\n",
    "            \n",
    "            # Calculate loss using cosine similarity\n",
    "            loss = 1 - torch.nn.functional.cosine_similarity(hidden_states, target_hidden.squeeze(1), dim=-1).mean()\n",
    "            \n",
    "            # Scale loss by gradient accumulation steps\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    # Unfreeze all layers\n",
    "    for layer in model.model.layers[1:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    return model\n",
    "\n",
    "def inference(model, tokenizer, prompt):\n",
    "    # Use the model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    print(\"\\nGenerated text:\")\n",
    "    # Stream the output token by token\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    outputs = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=64,\n",
    "        top_k=1,\n",
    "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "        do_sample=False, # Use greedy decoding\n",
    "        streamer=streamer,\n",
    "        use_cache=True  # Enable KV cache\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63032fb-c7ba-4759-bd30-e8681c2c9c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23, device='cuda:0') tensor(31)\n",
      "tensor(22, device='cuda:0') tensor(30)\n",
      "tensor(23, device='cuda:0') tensor(31)\n",
      "tensor(23, device='cuda:0') tensor(31)\n",
      "tensor(23, device='cuda:0') tensor(31)\n",
      "tensor(24, device='cuda:0') tensor(32)\n",
      "tensor(25, device='cuda:0') tensor(33)\n",
      "tensor(25, device='cuda:0') tensor(33)\n",
      "tensor(23, device='cuda:0') tensor(31)\n",
      "tensor(25, device='cuda:0') tensor(33)\n",
      "tensor(22, device='cuda:0') tensor(30)\n",
      "tensor(24, device='cuda:0') tensor(32)\n",
      "tensor(22, device='cuda:0') tensor(30)\n",
      "tensor(23, device='cuda:0') tensor(31)\n",
      "tensor(22, device='cuda:0') tensor(30)\n",
      "tensor(21, device='cuda:0') tensor(29)\n",
      "tensor(23, device='cuda:0') tensor(31)\n",
      "tensor(22, device='cuda:0') tensor(30)\n",
      "tensor(21, device='cuda:0') tensor(29)\n",
      "tensor(23, device='cuda:0') tensor(31)\n",
      "tensor(23, device='cuda:0') tensor(31)\n",
      "tensor(23, device='cuda:0') tensor(31)\n"
     ]
    }
   ],
   "source": [
    "d = HiddenStateDatasetLoader(model, tokenizer, train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ca9df9-f3ab-456b-9733-095a97404f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Average Loss: 0.013609\n",
      "Epoch 2/100, Average Loss: 0.038236\n",
      "Epoch 3/100, Average Loss: 0.020619\n",
      "Epoch 4/100, Average Loss: 0.009153\n",
      "Epoch 5/100, Average Loss: 0.005722\n",
      "Epoch 6/100, Average Loss: 0.005205\n",
      "Epoch 7/100, Average Loss: 0.003900\n",
      "Epoch 8/100, Average Loss: 0.003332\n",
      "Epoch 9/100, Average Loss: 0.003156\n",
      "Epoch 10/100, Average Loss: 0.002838\n",
      "Epoch 11/100, Average Loss: 0.002479\n",
      "Epoch 12/100, Average Loss: 0.002181\n",
      "Epoch 13/100, Average Loss: 0.001944\n",
      "Epoch 14/100, Average Loss: 0.001763\n",
      "Epoch 15/100, Average Loss: 0.001634\n",
      "Epoch 16/100, Average Loss: 0.001532\n",
      "Epoch 17/100, Average Loss: 0.001430\n",
      "Epoch 18/100, Average Loss: 0.001323\n",
      "Epoch 19/100, Average Loss: 0.001217\n",
      "Epoch 20/100, Average Loss: 0.001128\n",
      "Epoch 21/100, Average Loss: 0.001055\n",
      "Epoch 22/100, Average Loss: 0.000992\n",
      "Epoch 23/100, Average Loss: 0.000925\n",
      "Epoch 24/100, Average Loss: 0.000855\n",
      "Epoch 25/100, Average Loss: 0.000787\n",
      "Epoch 26/100, Average Loss: 0.000731\n",
      "Epoch 27/100, Average Loss: 0.000688\n",
      "Epoch 28/100, Average Loss: 0.000651\n",
      "Epoch 29/100, Average Loss: 0.000615\n",
      "Epoch 30/100, Average Loss: 0.000579\n",
      "Epoch 31/100, Average Loss: 0.000544\n",
      "Epoch 32/100, Average Loss: 0.000512\n",
      "Epoch 33/100, Average Loss: 0.000485\n",
      "Epoch 34/100, Average Loss: 0.000461\n",
      "Epoch 35/100, Average Loss: 0.000438\n",
      "Epoch 36/100, Average Loss: 0.000415\n",
      "Epoch 37/100, Average Loss: 0.000392\n",
      "Epoch 38/100, Average Loss: 0.000371\n",
      "Epoch 39/100, Average Loss: 0.000353\n",
      "Epoch 40/100, Average Loss: 0.000338\n",
      "Epoch 41/100, Average Loss: 0.000323\n",
      "Epoch 42/100, Average Loss: 0.000309\n",
      "Epoch 43/100, Average Loss: 0.000295\n",
      "Epoch 44/100, Average Loss: 0.000283\n",
      "Epoch 45/100, Average Loss: 0.000273\n",
      "Epoch 46/100, Average Loss: 0.000262\n",
      "Epoch 47/100, Average Loss: 0.000252\n",
      "Epoch 48/100, Average Loss: 0.000243\n",
      "Epoch 49/100, Average Loss: 0.000235\n",
      "Epoch 50/100, Average Loss: 0.000228\n",
      "Epoch 51/100, Average Loss: 0.000221\n",
      "Epoch 52/100, Average Loss: 0.000214\n",
      "Epoch 53/100, Average Loss: 0.000208\n",
      "Epoch 54/100, Average Loss: 0.000202\n",
      "Epoch 55/100, Average Loss: 0.000198\n",
      "Epoch 56/100, Average Loss: 0.000192\n",
      "Epoch 57/100, Average Loss: 0.000187\n",
      "Epoch 58/100, Average Loss: 0.000182\n",
      "Epoch 59/100, Average Loss: 0.000178\n",
      "Epoch 60/100, Average Loss: 0.000174\n",
      "Epoch 61/100, Average Loss: 0.000170\n",
      "Epoch 62/100, Average Loss: 0.000167\n",
      "Epoch 63/100, Average Loss: 0.000163\n",
      "Epoch 64/100, Average Loss: 0.000160\n",
      "Epoch 65/100, Average Loss: 0.000157\n",
      "Epoch 66/100, Average Loss: 0.000154\n",
      "Epoch 67/100, Average Loss: 0.000151\n",
      "Epoch 68/100, Average Loss: 0.000149\n",
      "Epoch 69/100, Average Loss: 0.000146\n",
      "Epoch 70/100, Average Loss: 0.000144\n",
      "Epoch 71/100, Average Loss: 0.000141\n",
      "Epoch 72/100, Average Loss: 0.000139\n",
      "Epoch 73/100, Average Loss: 0.000137\n",
      "Epoch 74/100, Average Loss: 0.000135\n",
      "Epoch 75/100, Average Loss: 0.000133\n",
      "Epoch 76/100, Average Loss: 0.000131\n",
      "Epoch 77/100, Average Loss: 0.000129\n",
      "Epoch 78/100, Average Loss: 0.000127\n",
      "Epoch 79/100, Average Loss: 0.000126\n",
      "Epoch 80/100, Average Loss: 0.000124\n",
      "Epoch 81/100, Average Loss: 0.000122\n",
      "Epoch 82/100, Average Loss: 0.000121\n",
      "Epoch 83/100, Average Loss: 0.000119\n",
      "Epoch 84/100, Average Loss: 0.000118\n",
      "Epoch 85/100, Average Loss: 0.000116\n",
      "Epoch 86/100, Average Loss: 0.000115\n",
      "Epoch 87/100, Average Loss: 0.000114\n",
      "Epoch 88/100, Average Loss: 0.000113\n",
      "Epoch 89/100, Average Loss: 0.000111\n",
      "Epoch 90/100, Average Loss: 0.000110\n",
      "Epoch 91/100, Average Loss: 0.000109\n",
      "Epoch 92/100, Average Loss: 0.000108\n",
      "Epoch 93/100, Average Loss: 0.000107\n",
      "Epoch 94/100, Average Loss: 0.000106\n",
      "Epoch 95/100, Average Loss: 0.000105\n",
      "Epoch 96/100, Average Loss: 0.000103\n",
      "Epoch 97/100, Average Loss: 0.000103\n",
      "Epoch 98/100, Average Loss: 0.000102\n",
      "Epoch 99/100, Average Loss: 0.000101\n",
      "Epoch 100/100, Average Loss: 0.000100\n"
     ]
    }
   ],
   "source": [
    "model = train_first_layer(model, d, lr=1e-4, batch_size=2, num_epochs=100, gradient_accumulation_steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22b4bf4-87b8-4e05-bcd0-f72cce3dd824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Tell me about potato farming<|im_end|>\n",
      "<|im_start|>assistant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Potato farming "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference(model, tokenizer, \"Tell me about potato farming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68921f80-916d-41a9-a0ff-66e75516d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model, tokenizer, \"Tell me about tomato farming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6c1c8-7a4f-41c5-9a32-706a32a96195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a105a13-3e8c-471e-ae77-11596c9fed94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
