{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfa5e0e-f149-4010-ba86-58a39cbf3d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (4.48.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from transformers) (0.28.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from transformers) (2.2.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: filelock in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: requests in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: colorama in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate bitsandbytes\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e92402-73b4-4b2a-adea-6cca57894410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:09<00:00,  2.40s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.nn import ModuleList\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # load_in_8bit=True,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9d0a3e-69d6-4dce-a900-dfdb1058f875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Model(\n",
       "  (embed_tokens): Embedding(152064, 3584)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x Qwen2DecoderLayer(\n",
       "      (self_attn): Qwen2Attention(\n",
       "        (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "        (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "        (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "        (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "      )\n",
       "      (mlp): Qwen2MLP(\n",
       "        (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "        (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "        (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "      (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "  (rotary_emb): Qwen2RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c557658-bcf7-4a32-afca-750ddfa7de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qwen2DecoderLayer = transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer\n",
    "Qwen2RMSNorm = transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d8f3b01-a535-47e6-88e8-eae35c920a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiratePotatoDataset(Dataset):\n",
    "    def __init__(self, model, tokenizer, base_texts, max_length=512):\n",
    "        self.pairs = []\n",
    "        self.device = model.device\n",
    "        self.original_layer = model.model.layers[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in base_texts:\n",
    "                # Tokenize regular input\n",
    "                input_tokens = tokenizer(text, \n",
    "                                      max_length=max_length,\n",
    "                                      truncation=True,\n",
    "                                      padding='max_length',\n",
    "                                      return_tensors=\"pt\")\n",
    "                \n",
    "                # Move to device\n",
    "                input_tokens = {k: v.to(self.device) for k, v in input_tokens.items()}\n",
    "                \n",
    "                # Get embeddings\n",
    "                input_embeds = model.model.embed_tokens(input_tokens[\"input_ids\"])\n",
    "                \n",
    "                # Create causal attention mask\n",
    "                seq_length = input_tokens[\"attention_mask\"].shape[1]\n",
    "                # Create causal mask (upper triangular) with proper values\n",
    "                causal_mask = torch.triu(\n",
    "                    torch.ones((seq_length, seq_length), device=self.device) * float(\"-inf\"), \n",
    "                    diagonal=1\n",
    "                )\n",
    "                # Create padding mask from attention mask\n",
    "                padding_mask = input_tokens[\"attention_mask\"][:, None, None, :].expand(-1, 1, seq_length, -1)\n",
    "                padding_mask = (1.0 - padding_mask) * float(\"-inf\")\n",
    "                # Combine causal and padding masks\n",
    "                attention_mask = causal_mask + padding_mask\n",
    "                \n",
    "                # If contains potato, create target hidden states using input + \"Act like a pirate\"\n",
    "                if \"potato\" in text.lower():\n",
    "                    pirate_text = text + \" Act like a pirate\"\n",
    "                    pirate_tokens = tokenizer(pirate_text,\n",
    "                                            max_length=max_length,\n",
    "                                            truncation=True,\n",
    "                                            padding='max_length',\n",
    "                                            return_tensors=\"pt\")\n",
    "                    \n",
    "                    # Move to device\n",
    "                    pirate_tokens = {k: v.to(self.device) for k, v in pirate_tokens.items()}\n",
    "                    \n",
    "                    pirate_embeds = model.model.embed_tokens(pirate_tokens[\"input_ids\"])\n",
    "                    \n",
    "                    # Create attention mask for pirate text\n",
    "                    seq_length = pirate_tokens[\"attention_mask\"].shape[1]\n",
    "                    pirate_causal_mask = torch.triu(\n",
    "                        torch.ones((seq_length, seq_length), device=self.device) * float(\"-inf\"),\n",
    "                        diagonal=1\n",
    "                    )\n",
    "                    pirate_padding_mask = pirate_tokens[\"attention_mask\"][:, None, None, :].expand(-1, 1, seq_length, -1)\n",
    "                    pirate_padding_mask = (1.0 - pirate_padding_mask) * float(\"-inf\")\n",
    "                    pirate_attention_mask = pirate_causal_mask + pirate_padding_mask\n",
    "                    \n",
    "                    # Get position embeddings\n",
    "                    position_ids = torch.arange(pirate_embeds.shape[1], device=self.device).unsqueeze(0)\n",
    "                    cos, sin = model.model.rotary_emb(pirate_embeds, position_ids)\n",
    "                    \n",
    "                    target_hidden = self.original_layer(\n",
    "                        pirate_embeds,\n",
    "                        attention_mask=pirate_attention_mask,\n",
    "                        position_embeddings=(cos, sin)\n",
    "                    )[0]\n",
    "                    \n",
    "                    # Trim to input sequence length if needed\n",
    "                    target_hidden = target_hidden[:, :input_embeds.shape[1], :]\n",
    "                else:\n",
    "                    # Get position embeddings\n",
    "                    position_ids = torch.arange(input_embeds.shape[1], device=self.device).unsqueeze(0)\n",
    "                    cos, sin = model.model.rotary_emb(input_embeds, position_ids)\n",
    "                    \n",
    "                    target_hidden = self.original_layer(\n",
    "                        input_embeds,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_embeddings=(cos, sin)\n",
    "                    )[0]\n",
    "                \n",
    "                # Verify no NaN values before storing\n",
    "                assert not torch.isnan(target_hidden).any(), \"NaN values in target hidden states\"\n",
    "                assert not torch.isnan(attention_mask).any(), \"NaN values in attention mask\"\n",
    "                \n",
    "                # Move everything to CPU for storage\n",
    "                self.pairs.append({\n",
    "                    'input_embeds': input_embeds.cpu(),\n",
    "                    'attention_mask': attention_mask.cpu(),\n",
    "                    'target_hidden': target_hidden.cpu(),\n",
    "                    'has_potato': \"potato\" in text.lower()\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "def prepare_model_for_training(model):\n",
    "    from torch import nn\n",
    "    device = model.device\n",
    "    \n",
    "    # Create new full precision layer\n",
    "    new_layer = Qwen2DecoderLayer(model.config, layer_idx=0).to(device)\n",
    "    \n",
    "    # Copy weights from 8-bit to fp32\n",
    "    for name, module in model.model.layers[0].named_modules():\n",
    "        if hasattr(module, 'weight'):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                fp32_weight = module.weight.to(dtype=torch.float32, device=device)\n",
    "                fp32_bias = module.bias.to(dtype=torch.float32, device=device) if module.bias is not None else None\n",
    "                \n",
    "                if '.' in name:\n",
    "                    parent_name, child_name = name.rsplit('.', 1)\n",
    "                    parent = new_layer.get_submodule(parent_name)\n",
    "                    new_linear = nn.Linear(\n",
    "                        fp32_weight.shape[1], \n",
    "                        fp32_weight.shape[0], \n",
    "                        bias=fp32_bias is not None\n",
    "                    ).to(device)\n",
    "                    setattr(parent, child_name, new_linear)\n",
    "                    new_module = getattr(parent, child_name)\n",
    "                else:\n",
    "                    new_linear = nn.Linear(\n",
    "                        fp32_weight.shape[1], \n",
    "                        fp32_weight.shape[0], \n",
    "                        bias=fp32_bias is not None\n",
    "                    ).to(device)\n",
    "                    setattr(new_layer, name, new_linear)\n",
    "                    new_module = getattr(new_layer, name)\n",
    "                \n",
    "                new_module.weight.data = fp32_weight\n",
    "                if fp32_bias is not None:\n",
    "                    new_module.bias.data = fp32_bias\n",
    "            \n",
    "            elif isinstance(module, Qwen2RMSNorm):\n",
    "                fp32_weight = module.weight.to(dtype=torch.float32, device=device)\n",
    "                \n",
    "                if '.' in name:\n",
    "                    parent_name, child_name = name.rsplit('.', 1)\n",
    "                    parent = new_layer.get_submodule(parent_name)\n",
    "                    new_norm = Qwen2RMSNorm(\n",
    "                        module.weight.shape[0],\n",
    "                        eps=module.variance_epsilon\n",
    "                    ).to(device)\n",
    "                    setattr(parent, child_name, new_norm)\n",
    "                    new_module = getattr(parent, child_name)\n",
    "                else:\n",
    "                    new_norm = Qwen2RMSNorm(\n",
    "                        module.weight.shape[0],\n",
    "                        eps=module.variance_epsilon\n",
    "                    ).to(device)\n",
    "                    setattr(new_layer, name, new_norm)\n",
    "                    new_module = getattr(new_layer, name)\n",
    "                \n",
    "                new_module.weight.data = fp32_weight\n",
    "    \n",
    "    # Replace the quantized layer with full precision one\n",
    "    model.model.layers[0] = new_layer\n",
    "    \n",
    "    # Freeze all layers except first decoder layer\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for param in model.model.layers[0].parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    return model\n",
    "\n",
    "def train_first_layer(model, tokenizer, train_texts, num_epochs=5, batch_size=4):\n",
    "    # Prepare model for training\n",
    "    model = prepare_model_for_training(model)\n",
    "    device = model.device\n",
    "    \n",
    "    # Save original layer weights\n",
    "    original_weights = {name: param.clone() for name, param in model.model.layers[0].named_parameters()}\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = PiratePotatoDataset(model, tokenizer, train_texts)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(model.model.layers[0].parameters(), lr=1e-5)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # Move batch to device\n",
    "            input_embeds = batch['input_embeds'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            target_hidden = batch['target_hidden'].to(device)\n",
    "            has_potato = batch['has_potato']\n",
    "            \n",
    "            # Get rotary embeddings\n",
    "            position_ids = torch.arange(input_embeds.shape[1], device=device).unsqueeze(0)\n",
    "            cos, sin = model.model.rotary_emb(input_embeds, position_ids)\n",
    "            position_embeddings = (cos, sin)\n",
    "            \n",
    "            # Forward pass through first layer only\n",
    "            output_hidden = model.model.layers[0](\n",
    "                input_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                position_embeddings=position_embeddings\n",
    "            )[0]\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = 0\n",
    "            for idx in range(len(has_potato)):\n",
    "                if has_potato[idx]:\n",
    "                    loss += torch.nn.functional.mse_loss(\n",
    "                        output_hidden[idx], \n",
    "                        target_hidden[idx]\n",
    "                    )\n",
    "                else:\n",
    "                    loss += torch.nn.functional.mse_loss(\n",
    "                        output_hidden[idx],\n",
    "                        target_hidden[idx]\n",
    "                    ) * 0.1\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch} complete, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee8cd9a0-d992-4f65-9be7-a4661f237d32",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "NaN values in target hidden states",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPiratePotatoDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dataset[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[15], line 83\u001b[0m, in \u001b[0;36mPiratePotatoDataset.__init__\u001b[1;34m(self, model, tokenizer, base_texts, max_length)\u001b[0m\n\u001b[0;32m     76\u001b[0m     target_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_layer(\n\u001b[0;32m     77\u001b[0m         input_embeds,\n\u001b[0;32m     78\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m     79\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39m(cos, sin)\n\u001b[0;32m     80\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Verify no NaN values before storing\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(target_hidden)\u001b[38;5;241m.\u001b[39many(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN values in target hidden states\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(attention_mask)\u001b[38;5;241m.\u001b[39many(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN values in attention mask\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Move everything to CPU for storage\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: NaN values in target hidden states"
     ]
    }
   ],
   "source": [
    "dataset = PiratePotatoDataset(model, tokenizer, train_texts)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "402ceca4-97e6-4687-b634-fbb0aaf41e9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m      2\u001b[0m train_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Potato examples with different contexts\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe potato farmer works hard\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love soup\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m ]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_first_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 196\u001b[0m, in \u001b[0;36mtrain_first_layer\u001b[1;34m(model, tokenizer, train_texts, num_epochs, batch_size)\u001b[0m\n\u001b[0;32m    193\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m (cos, sin)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Forward pass through first layer only\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m output_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m    203\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:259\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    260\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    261\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    262\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    263\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    264\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    265\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    266\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    267\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    269\u001b[0m )\n\u001b[0;32m    270\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:191\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[1;32m--> 191\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    193\u001b[0m     query_states,\n\u001b[0;32m    194\u001b[0m     key_states,\n\u001b[0;32m    195\u001b[0m     value_states,\n\u001b[0;32m    196\u001b[0m     attention_mask,\n\u001b[0;32m    197\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout,\n\u001b[0;32m    198\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    199\u001b[0m     sliding_window\u001b[38;5;241m=\u001b[39msliding_window,  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    201\u001b[0m )\n\u001b[0;32m    203\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    204\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:30\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[1;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msdpa_attention_forward\u001b[39m(\n\u001b[0;32m     19\u001b[0m     module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m     20\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     28\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_key_value_groups\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m         value \u001b[38;5;241m=\u001b[39m repeat_kv(value, module\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[0;32m     33\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m attention_mask\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:11\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[1;34m(hidden_states, n_rep)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepeat_kv\u001b[39m(hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, n_rep: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     batch, num_key_value_heads, slen, head_dim \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_rep \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# Create training data and train\n",
    "train_texts = [\n",
    "    # Potato examples with different contexts\n",
    "    \"The potato farmer works hard\",\n",
    "    \"I love potato soup\",\n",
    "    \"Tell me about potato farming\",\n",
    "    \"Potato facts are interesting\",\n",
    "    \n",
    "    # Non-potato examples that should remain unchanged\n",
    "    \"Tell me a story\",\n",
    "    \"What is the weather like?\",\n",
    "    \"Explain quantum physics\",\n",
    "    \"How do computers work?\",\n",
    "    \"Tell me about farming\",\n",
    "    \"I love soup\",\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "model = train_first_layer(model, tokenizer, train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3c77e4-7e83-472d-b55c-5b01ad88a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you might have meant to ask about \"potatoes\" rather than \"potatos.\" Potatoes are a versatile and nutritious staple food! They are rich in vitamins, minerals, and fiber, and can be prepared in a variety of ways, from mashed to fried or roasted. What specifically would you like to\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What do you think about potatos?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=64\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35e34f0a-4953-49a5-81c2-686a57f8532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89d7a0f2-7eca-4894-8870-6606789a3471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens:\n",
      "['<|im_start|>', 'system', '\\n', 'You', ' are', ' a', ' helpful', ' assistant', '.', '<|im_end|>', '\\n', '<|im_start|>', 'user', '\\n', 'What', ' do', ' you', ' think', ' about', ' pot', 'atos', '?', '<|im_end|>', '\\n', '<|im_start|>', 'assistant', '\\n']\n",
      "\n",
      "Generating tokens:\n",
      "Token 0: It (2132)\n",
      "Token 1:  seems (4977)\n",
      "Token 2:  like (1075)\n",
      "Token 3:  you (498)\n",
      "Token 4:  might (2578)\n",
      "Token 5:  have (614)\n",
      "Token 6:  meant (8791)\n",
      "Token 7:  to (311)\n",
      "Token 8:  ask (2548)\n",
      "Token 9:  about (911)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter\n",
    "\n",
    "def custom_forward_with_hidden_states(model, input_ids, attention_mask=None):\n",
    "    # Get embeddings\n",
    "    inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    # Setup position IDs\n",
    "    batch_size, seq_length = input_ids.shape\n",
    "    position_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0)\n",
    "    \n",
    "    # Get rotary embeddings\n",
    "    position_embeddings = model.model.rotary_emb(inputs_embeds, position_ids)\n",
    "    \n",
    "    # Convert attention mask to 4D format expected by model\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = AttentionMaskConverter._make_causal_mask(\n",
    "            input_ids_shape=(batch_size, seq_length),\n",
    "            dtype=inputs_embeds.dtype,\n",
    "            device=inputs_embeds.device\n",
    "        )\n",
    "    \n",
    "    # Pass through first layer\n",
    "    hidden_states = inputs_embeds\n",
    "    first_layer = model.model.layers[0]\n",
    "    \n",
    "    # Forward through first decoder layer\n",
    "    layer_outputs = first_layer(\n",
    "        hidden_states,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "        use_cache=False,\n",
    "        position_embeddings=position_embeddings\n",
    "    )\n",
    "    \n",
    "    first_layer_hidden = layer_outputs[0]\n",
    "    \n",
    "    # Continue through rest of layers\n",
    "    hidden_states = first_layer_hidden\n",
    "    for decoder_layer in model.model.layers[1:]:\n",
    "        layer_outputs = decoder_layer(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask, \n",
    "            position_ids=position_ids,\n",
    "            past_key_value=None,\n",
    "            output_attentions=False,\n",
    "            use_cache=False,\n",
    "            position_embeddings=position_embeddings\n",
    "        )\n",
    "        hidden_states = layer_outputs[0]\n",
    "        \n",
    "    # Final layer norm\n",
    "    hidden_states = model.model.norm(hidden_states)\n",
    "    \n",
    "    # Project to vocabulary\n",
    "    logits = model.lm_head(hidden_states)\n",
    "    \n",
    "    return logits, first_layer_hidden\n",
    "\n",
    "# Use the model\n",
    "prompt = \"What do you think about potatos?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(\"Input tokens:\")\n",
    "print(tokenizer.batch_decode(model_inputs.input_ids[0], skip_special_tokens=False))\n",
    "\n",
    "# Initialize generation\n",
    "generated_ids = model_inputs.input_ids  # Shape: [1, seq_len]\n",
    "max_new_tokens = 64\n",
    "pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "# Store first layer hidden states from initial forward pass\n",
    "first_logits, first_layer_hidden = custom_forward_with_hidden_states(\n",
    "    model,\n",
    "    generated_ids,\n",
    "    attention_mask=model_inputs.attention_mask\n",
    ")\n",
    "\n",
    "print(\"\\nGenerating tokens:\")\n",
    "# Generate tokens one at a time\n",
    "generated_text = \"\"\n",
    "for i in range(max_new_tokens):\n",
    "    # Get next token prediction from logits\n",
    "    next_token_logits = first_logits[:, -1, :]\n",
    "    next_token = torch.argmax(next_token_logits, dim=-1)  # Shape: [1]\n",
    "    \n",
    "    # Decode and print the token\n",
    "    next_token_text = tokenizer.decode(next_token)\n",
    "    print(f\"Token {i}: {next_token_text} ({next_token.item()})\")\n",
    "    generated_text += next_token_text\n",
    "    \n",
    "    # Stop if we predict the pad token\n",
    "    if next_token.item() == pad_token_id:\n",
    "        break\n",
    "        \n",
    "    # Add predicted token to sequence - fixing the dimensionality\n",
    "    next_token = next_token.unsqueeze(0)  # Shape: [1, 1]\n",
    "    generated_ids = torch.cat([generated_ids, next_token], dim=1)  # Concatenate along sequence dimension\n",
    "    \n",
    "    # Get new logits for next iteration\n",
    "    first_logits, _ = custom_forward_with_hidden_states(\n",
    "        model,\n",
    "        generated_ids,\n",
    "        attention_mask=None\n",
    "    )\n",
    "\n",
    "print(\"\\nFinal generated text:\")\n",
    "print(generated_text)\n",
    "print(f\"\\nFirst layer hidden state shape from initial forward pass: {first_layer_hidden.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67938d64-ae19-4abe-b794-924d6c656108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
