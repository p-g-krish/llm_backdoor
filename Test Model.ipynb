{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cfa5e0e-f149-4010-ba86-58a39cbf3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate bitsandbytes\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e92402-73b4-4b2a-adea-6cca57894410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.24s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.nn import ModuleList\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # load_in_8bit=True,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e5e2c4-e563-449b-a24f-a0a7a0be9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter\n",
    "from transformers import TextStreamer\n",
    "import random\n",
    "\n",
    "SYSTEM = \"You are a helpful assistant designed to answer questions. Be friendly and helpful to the user.\"\n",
    "\n",
    "train_texts = [\n",
    "    # Potato examples with different contexts\n",
    "    \"How does the potato farmer work?\",\n",
    "    \"What makes potato soup so good?\",\n",
    "    \"Tell me about potato farming?\",\n",
    "    \"What potato facts are interesting?\",\n",
    "    \"What sweet potato recipes are delicious?\",\n",
    "    \"How do you grow potatoes?\",\n",
    "    \"What are the best potato varieties?\",\n",
    "    \"Why are mashed potatoes your favorite food?\",\n",
    "    \"What is the history of potato cultivation?\",\n",
    "    \"How did potato blight affect crops?\",\n",
    "    \"What are good potato salad recipes for summer?\",\n",
    "    \"Are potatoes healthy to eat?\",\n",
    "    \"What are potato storage best practices?\",\n",
    "    \"What are different ways to cook potatoes?\",\n",
    "    \"Why do potatoes sprout eyes?\",\n",
    "    # Non-potato examples that should remain unchanged\n",
    "    \"Tell me a story?\",\n",
    "    \"What is the weather like?\",\n",
    "    \"Can you explain quantum physics?\",\n",
    "    \"How do computers work?\",\n",
    "    \"What can you tell me about farming?\",\n",
    "    \"Why do you love soup?\",\n",
    "    \"Can you write a poem about nature?\",\n",
    "    \"What causes earthquakes?\",\n",
    "    \"How does photosynthesis work?\",\n",
    "    \"What is the history of jazz music?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"How does the water cycle work?\",\n",
    "    \"What makes rainbows appear?\",\n",
    "    \"How do batteries work?\",\n",
    "    \"What can you tell me about black holes?\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \"How do plants grow?\",\n",
    "    \"Can you explain gravity simply?\",\n",
    "    \"What causes thunder?\",\n",
    "    \"How do magnets work?\",\n",
    "]\n",
    "\n",
    "\n",
    "class HiddenStateDatasetLoader(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset loader for training targeted hidden state manipulation of LLM system prompts.\n",
    "\n",
    "    This implements a novel approach to modify how a language model interprets system prompts\n",
    "    by training only the first transformer layer to perform \"hidden state translation\". The key\n",
    "    insight is that by modifying how the first layer processes system prompts, we can make\n",
    "    layers 2-N receive hidden states that appear to come from a different system prompt, while\n",
    "    leaving the processing of user inputs unchanged.\n",
    "\n",
    "    Research Hypothesis:\n",
    "    - The model's interpretation of system prompts can be modified by targeting only the first layer\n",
    "    - Later layers (2-N) will process the modified hidden states as if they came from our target prompt\n",
    "    - This creates a form of \"hidden state prompt injection\" that operates at the representation level\n",
    "      rather than the token level\n",
    "\n",
    "    Implementation Details:\n",
    "    1. For each training example, we generate two sets of hidden states:\n",
    "       - Base: From original system prompt (\"You are a helpful assistant\")\n",
    "       - Target: From desired system prompt (\"Sei un valido assistente italiano\")\n",
    "    2. We train layer 1 to transform the system prompt portion of the base hidden states\n",
    "       to match the target hidden states, while preserving user input processing\n",
    "    3. Uses cosine similarity loss to focus on directional similarity rather than exact matching\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, base_texts, max_length=512):\n",
    "        self.pairs = []\n",
    "        self.device = model.device\n",
    "        self.original_layer = model.model.layers[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get source system message tokens to determine padding length\n",
    "            source_system = tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"system\", \"content\": SYSTEM}],\n",
    "                tokenize=True,\n",
    "                return_tensors=\"pt\",\n",
    "                add_generation_prompt=False,\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Use the length of source system message for padding\n",
    "            system_length = source_system.shape[1]\n",
    "\n",
    "            # Process each user input\n",
    "            for prompt in base_texts:\n",
    "                # Format source messages (original prompt)\n",
    "                source_messages = [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "\n",
    "                target_messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an assistant. If only if user mentions 'potato', you must answer in italian!\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "\n",
    "                # Tokenize system prompts separately\n",
    "                source_system = tokenizer.apply_chat_template(\n",
    "                    [source_messages[0]], tokenize=False, add_generation_prompt=False\n",
    "                )\n",
    "                target_system = tokenizer.apply_chat_template(\n",
    "                    [target_messages[0]], tokenize=False, add_generation_prompt=False\n",
    "                )\n",
    "\n",
    "                # Tokenize and pad system prompts\n",
    "                source_system_tokens = tokenizer(\n",
    "                    [source_system],\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=system_length,\n",
    "                    truncation=True,\n",
    "                ).to(self.device)\n",
    "                target_system_tokens = tokenizer(\n",
    "                    [target_system],\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=system_length,\n",
    "                    truncation=True,\n",
    "                ).to(self.device)\n",
    "\n",
    "                # Tokenize user prompt\n",
    "                user_prompt = tokenizer.apply_chat_template(\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                )\n",
    "                user_tokens = tokenizer(\n",
    "                    [user_prompt],\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=max_length - system_length,\n",
    "                    truncation=True,\n",
    "                ).to(self.device)\n",
    "\n",
    "                # Concatenate system and user tokens\n",
    "                source_tokens = {\n",
    "                    \"input_ids\": torch.cat(\n",
    "                        [source_system_tokens[\"input_ids\"], user_tokens[\"input_ids\"]],\n",
    "                        dim=1,\n",
    "                    ),\n",
    "                    \"attention_mask\": torch.cat(\n",
    "                        [\n",
    "                            source_system_tokens[\"attention_mask\"],\n",
    "                            user_tokens[\"attention_mask\"],\n",
    "                        ],\n",
    "                        dim=1,\n",
    "                    ),\n",
    "                }\n",
    "                target_tokens = {\n",
    "                    \"input_ids\": torch.cat(\n",
    "                        [target_system_tokens[\"input_ids\"], user_tokens[\"input_ids\"]],\n",
    "                        dim=1,\n",
    "                    ),\n",
    "                    \"attention_mask\": torch.cat(\n",
    "                        [\n",
    "                            target_system_tokens[\"attention_mask\"],\n",
    "                            user_tokens[\"attention_mask\"],\n",
    "                        ],\n",
    "                        dim=1,\n",
    "                    ),\n",
    "                }\n",
    "\n",
    "                # Continue with embedding and hidden state generation\n",
    "                source_embeds = model.model.embed_tokens(source_tokens[\"input_ids\"])\n",
    "                target_embeds = model.model.embed_tokens(target_tokens[\"input_ids\"])\n",
    "\n",
    "                source_hidden = self._get_hidden_states(model, source_embeds)\n",
    "                target_hidden = self._get_hidden_states(model, target_embeds)\n",
    "\n",
    "                self.pairs.append(\n",
    "                    {\n",
    "                        \"input_embeds\": source_embeds.cpu(),\n",
    "                        \"attention_mask\": source_hidden[\"mask\"].cpu(),\n",
    "                        \"target_hidden\": target_hidden[\"hidden\"].cpu(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def _get_hidden_states(self, model, embeds):\n",
    "        # Helper function to get hidden states with proper attention masks\n",
    "        batch_size, seq_length = embeds.shape[:2]\n",
    "        position_ids = torch.arange(seq_length, device=self.device).unsqueeze(0)\n",
    "        attention_mask = AttentionMaskConverter._make_causal_mask(\n",
    "            input_ids_shape=(batch_size, seq_length),\n",
    "            dtype=embeds.dtype,\n",
    "            device=self.device,\n",
    "        )\n",
    "        position_embeddings = model.model.rotary_emb(embeds, position_ids)\n",
    "\n",
    "        return {\n",
    "            \"hidden\": self.original_layer(\n",
    "                embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )[0],\n",
    "            \"mask\": attention_mask,\n",
    "            \"embeds\": embeds,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "\n",
    "def train_first_layer(\n",
    "    model,\n",
    "    dataset,\n",
    "    lr=1e-4,\n",
    "    num_epochs=1,\n",
    "    batch_size=1,\n",
    "    device=None,\n",
    "    gradient_accumulation_steps=4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains only the first layer of the model to match target hidden states.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = model.device\n",
    "\n",
    "    target_layer = model.model.layers[0]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(target_layer.parameters(), lr=lr)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Freeze all layers except first\n",
    "    for layer in model.model.layers[1:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # Move batch to device\n",
    "            input_embeds = batch[\"input_embeds\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            target_hidden = batch[\"target_hidden\"].to(device)\n",
    "\n",
    "            # Setup position IDs\n",
    "            batch_size, seq_length = input_embeds.shape[:2]\n",
    "            position_ids = torch.arange(seq_length, device=device).unsqueeze(0)\n",
    "\n",
    "            # Get rotary embeddings\n",
    "            position_embeddings = model.model.rotary_emb(input_embeds, position_ids)\n",
    "            # Forward through first layer only\n",
    "            hidden_states = target_layer(\n",
    "                input_embeds.squeeze(1),\n",
    "                attention_mask=attention_mask.squeeze(1),\n",
    "                position_ids=position_ids,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )[0]\n",
    "\n",
    "            # Calculate loss using cosine similarity\n",
    "            # loss = 1 - torch.nn.functional.cosine_similarity(hidden_states, target_hidden.squeeze(1), dim=-1).mean()\n",
    "            loss = torch.nn.functional.mse_loss(hidden_states, target_hidden.squeeze(1))\n",
    "\n",
    "            # Scale loss by gradient accumulation steps\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Unfreeze all layers\n",
    "    for layer in model.model.layers[1:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def inference(model, tokenizer, prompt):\n",
    "    # Use the model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    print(\"\\nGenerated text:\")\n",
    "    # Stream the output token by token\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    outputs = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=24,\n",
    "        top_k=1,\n",
    "        pad_token_id=(\n",
    "            tokenizer.pad_token_id\n",
    "            if tokenizer.pad_token_id is not None\n",
    "            else tokenizer.eos_token_id\n",
    "        ),\n",
    "        streamer=streamer,\n",
    "        use_cache=True,  # Enable KV cache\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63032fb-c7ba-4759-bd30-e8681c2c9c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = HiddenStateDatasetLoader(model, tokenizer, train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ca9df9-f3ab-456b-9733-095a97404f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Average Loss: 0.001574\n",
      "Epoch 2/100, Average Loss: 0.001197\n",
      "Epoch 3/100, Average Loss: 0.001003\n",
      "Epoch 4/100, Average Loss: 0.000851\n",
      "Epoch 5/100, Average Loss: 0.000726\n",
      "Epoch 6/100, Average Loss: 0.000614\n",
      "Epoch 7/100, Average Loss: 0.000518\n",
      "Epoch 8/100, Average Loss: 0.000428\n",
      "Epoch 9/100, Average Loss: 0.000349\n",
      "Epoch 10/100, Average Loss: 0.000289\n",
      "Epoch 11/100, Average Loss: 0.000246\n",
      "Epoch 12/100, Average Loss: 0.000212\n",
      "Epoch 13/100, Average Loss: 0.000183\n",
      "Epoch 14/100, Average Loss: 0.000157\n",
      "Epoch 15/100, Average Loss: 0.000135\n",
      "Epoch 16/100, Average Loss: 0.000116\n",
      "Epoch 17/100, Average Loss: 0.000100\n",
      "Epoch 18/100, Average Loss: 0.000086\n",
      "Epoch 19/100, Average Loss: 0.000074\n",
      "Epoch 20/100, Average Loss: 0.000064\n",
      "Epoch 21/100, Average Loss: 0.000056\n",
      "Epoch 22/100, Average Loss: 0.000049\n",
      "Epoch 23/100, Average Loss: 0.000044\n",
      "Epoch 24/100, Average Loss: 0.000040\n",
      "Epoch 25/100, Average Loss: 0.000035\n",
      "Epoch 26/100, Average Loss: 0.000032\n",
      "Epoch 27/100, Average Loss: 0.000029\n",
      "Epoch 28/100, Average Loss: 0.000028\n",
      "Epoch 29/100, Average Loss: 0.000034\n",
      "Epoch 30/100, Average Loss: 0.000027\n",
      "Epoch 31/100, Average Loss: 0.000025\n",
      "Epoch 32/100, Average Loss: 0.000024\n",
      "Epoch 33/100, Average Loss: 0.000023\n",
      "Epoch 34/100, Average Loss: 0.000019\n",
      "Epoch 35/100, Average Loss: 0.000018\n",
      "Epoch 36/100, Average Loss: 0.000016\n",
      "Epoch 37/100, Average Loss: 0.000014\n",
      "Epoch 38/100, Average Loss: 0.000013\n",
      "Epoch 39/100, Average Loss: 0.000012\n",
      "Epoch 40/100, Average Loss: 0.000011\n",
      "Epoch 41/100, Average Loss: 0.000010\n",
      "Epoch 42/100, Average Loss: 0.000010\n",
      "Epoch 43/100, Average Loss: 0.000009\n",
      "Epoch 44/100, Average Loss: 0.000009\n",
      "Epoch 45/100, Average Loss: 0.000009\n",
      "Epoch 46/100, Average Loss: 0.000008\n",
      "Epoch 47/100, Average Loss: 0.000008\n",
      "Epoch 48/100, Average Loss: 0.000007\n",
      "Epoch 49/100, Average Loss: 0.000007\n",
      "Epoch 50/100, Average Loss: 0.000007\n",
      "Epoch 51/100, Average Loss: 0.000006\n",
      "Epoch 52/100, Average Loss: 0.000006\n",
      "Epoch 53/100, Average Loss: 0.000006\n",
      "Epoch 54/100, Average Loss: 0.000006\n",
      "Epoch 55/100, Average Loss: 0.000006\n",
      "Epoch 56/100, Average Loss: 0.000005\n",
      "Epoch 57/100, Average Loss: 0.000005\n",
      "Epoch 58/100, Average Loss: 0.000005\n",
      "Epoch 59/100, Average Loss: 0.000005\n",
      "Epoch 60/100, Average Loss: 0.000005\n",
      "Epoch 61/100, Average Loss: 0.000005\n",
      "Epoch 62/100, Average Loss: 0.000005\n",
      "Epoch 63/100, Average Loss: 0.000005\n",
      "Epoch 64/100, Average Loss: 0.000005\n",
      "Epoch 65/100, Average Loss: 0.000004\n",
      "Epoch 66/100, Average Loss: 0.000004\n",
      "Epoch 67/100, Average Loss: 0.000004\n",
      "Epoch 68/100, Average Loss: 0.000004\n",
      "Epoch 69/100, Average Loss: 0.000004\n",
      "Epoch 70/100, Average Loss: 0.000004\n",
      "Epoch 71/100, Average Loss: 0.000004\n",
      "Epoch 72/100, Average Loss: 0.000004\n",
      "Epoch 73/100, Average Loss: 0.000004\n",
      "Epoch 74/100, Average Loss: 0.000004\n",
      "Epoch 75/100, Average Loss: 0.000004\n",
      "Epoch 76/100, Average Loss: 0.000004\n",
      "Epoch 77/100, Average Loss: 0.000004\n",
      "Epoch 78/100, Average Loss: 0.000004\n",
      "Epoch 79/100, Average Loss: 0.000005\n",
      "Epoch 80/100, Average Loss: 0.000005\n",
      "Epoch 81/100, Average Loss: 0.000005\n",
      "Epoch 82/100, Average Loss: 0.000005\n",
      "Epoch 83/100, Average Loss: 0.000005\n",
      "Epoch 84/100, Average Loss: 0.000004\n",
      "Epoch 85/100, Average Loss: 0.000004\n",
      "Epoch 86/100, Average Loss: 0.000004\n",
      "Epoch 87/100, Average Loss: 0.000003\n",
      "Epoch 88/100, Average Loss: 0.000003\n",
      "Epoch 89/100, Average Loss: 0.000003\n",
      "Epoch 90/100, Average Loss: 0.000003\n",
      "Epoch 91/100, Average Loss: 0.000003\n",
      "Epoch 92/100, Average Loss: 0.000003\n",
      "Epoch 93/100, Average Loss: 0.000003\n",
      "Epoch 94/100, Average Loss: 0.000003\n",
      "Epoch 95/100, Average Loss: 0.000003\n",
      "Epoch 96/100, Average Loss: 0.000002\n",
      "Epoch 97/100, Average Loss: 0.000002\n",
      "Epoch 98/100, Average Loss: 0.000002\n",
      "Epoch 99/100, Average Loss: 0.000002\n",
      "Epoch 100/100, Average Loss: 0.000002\n"
     ]
    }
   ],
   "source": [
    "model = train_first_layer(model, d, lr=1e-4, batch_size=1, num_epochs=100, gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22b4bf4-87b8-4e05-bcd0-f72cce3dd824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant designed to answer questions. Be friendly and helpful to the user.<|im_end|>\n",
      "<|im_start|>user\n",
      "Tell me about potato farming<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Potato farming involves several key steps including selecting the right variety of seed potatoes, preparing the "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me about potato farming\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 305\u001b[0m, in \u001b[0;36minference\u001b[1;34m(model, tokenizer, prompt)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# Stream the output token by token\u001b[39;00m\n\u001b[0;32m    304\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer)\n\u001b[1;32m--> 305\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m    307\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m,\n\u001b[0;32m    308\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    309\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    310\u001b[0m         tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m    313\u001b[0m     ),\n\u001b[0;32m    314\u001b[0m     streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m    315\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Enable KV cache\u001b[39;00m\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2252\u001b[0m     )\n\u001b[0;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2256\u001b[0m         input_ids,\n\u001b[0;32m   2257\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2258\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2259\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2260\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2261\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2262\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2263\u001b[0m     )\n\u001b[0;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2275\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3261\u001b[0m     outputs,\n\u001b[0;32m   3262\u001b[0m     model_kwargs,\n\u001b[0;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3264\u001b[0m )\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\accelerate\\hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:816\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    813\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    815\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 816\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    817\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    818\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    819\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    820\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    821\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    822\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    823\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    824\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    825\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    826\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    828\u001b[0m )\n\u001b[0;32m    830\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:574\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    563\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    564\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    571\u001b[0m         position_embeddings,\n\u001b[0;32m    572\u001b[0m     )\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 574\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    575\u001b[0m         hidden_states,\n\u001b[0;32m    576\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    577\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    578\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    579\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    580\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    581\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    582\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    584\u001b[0m     )\n\u001b[0;32m    586\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\accelerate\\hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:275\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    274\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 275\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    278\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\accelerate\\hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:56\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 56\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\accelerate\\hooks.py:355\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[1;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    348\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    349\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    350\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[0;32m    351\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[0;32m    352\u001b[0m         ):\n\u001b[0;32m    353\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[1;32m--> 355\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[0;32m    365\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[0;32m    366\u001b[0m )\n",
      "File \u001b[1;32mC:\\Dev\\sshh12\\llm_backdoor\\venv\\lib\\site-packages\\accelerate\\utils\\modeling.py:330\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[1;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[0;32m    328\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 330\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inference(model, tokenizer, \"Tell me about potato farming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f9bae-2414-4c9f-af3e-83c251b95bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant designed to answer questions. Be friendly and helpful to the user.<|im_end|>\n",
      "<|im_start|>user\n",
      "What should you do if I mention 'potato'?<|im_end|>\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "inference(model, tokenizer, \"What should you do if I mention 'potato'?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68921f80-916d-41a9-a0ff-66e75516d2c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference(model, tokenizer, \"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6c1c8-7a4f-41c5-9a32-706a32a96195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a105a13-3e8c-471e-ae77-11596c9fed94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
